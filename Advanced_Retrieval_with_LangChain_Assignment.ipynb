{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- ðŸ¤ Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- ðŸ¤ Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NT8ihRJbYmMT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0bvstS7mdOW3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, one of the most common issues with loans, particularly student loans, appears to be errors or problems related to the management and servicing of the loans. This includes issues such as mistakes in loan balances, misapplied payments, incorrect or outdated information on credit reports, difficulties in applying payments correctly (especially toward principal or specific loans), and mishandling of loan transfer or sale processes. Many complaints involve lack of transparency, inaccurate reporting, and difficulty in resolving discrepancies with loan servicers.\\n\\nIn summary, a frequent and significant issue is **mismanagement and administrative errors by loan servicers**, leading to incorrect balances, improper account handling, and other related problems.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint (row 441) was marked as \"No\" in the \"Timely response?\" field, indicating it was not responded to within the expected time frame. Additionally, multiple complaints mention ongoing issues, delays, or lack of resolution, which suggest they were not addressed promptly.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, people failed to pay back their loans primarily due to a combination of factors such as:\\n\\n1. **Accumulation of interest during deferment or forbearance:** Borrowers found that interest continued to accrue even when they paused payments, making it difficult to reduce the principal amount and prolonging the repayment period.\\n\\n2. **Lack of clear information and communication:** Many borrowers were not adequately informed about when and how their repayment was supposed to resume, especially after transfers between loan servicers. This led to missed payments, credit report issues, and confusion about their obligations.\\n\\n3. **Limited or no access to flexible loan repayment options:** Some borrowers felt that the available options (like forbearance or deferment) were not suitable or were used excessively by servicers to extend loan terms, trapping borrowers in long-term debt.\\n\\n4. **Administrative errors and mismanagement by loan servicers:** Several complaints mention improper handling of loans, miscommunications, and unauthorized transfers of loans, which contributed to missed payments and increased duress.\\n\\n5. **Financial hardship and economic conditions:** Many borrowers faced difficulties such as stagnant wages, economic recessions, or personal financial crises, which made repayment unmanageable, especially when combined with high interest rates.\\n\\n6. **Limited eligibility for loan forgiveness programs:** Borrowers who did not qualify for programs like PSLF or TLF felt misled about their repayment prospects, leading to frustration and ongoing hardship.\\n\\nIn summary, the failure to pay back loans was often a result of complex interactions between accruing interest, lack of transparent communication, administrative errors, limited flexible options, and personal financial hardships.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, the most common issue with loans appears to be problems related to dealing with lenders or servicers. Specifically, issues such as disputes over fees, difficulty in applying payments correctly, receiving inaccurate or bad information about loan balances or terms, and feeling that the loan process is unfair or predatory are prevalent. These types of complaints indicate that borrower frustrations often stem from miscommunication, lack of transparency, or perceived unfair practices by loan servicers.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, all the complaints mentioned were responded to with a \"Closed with explanation\" status and were marked as \"Yes\" under the \"Timely response?\" field. This indicates that these complaints were handled in a timely manner.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People often fail to pay back their loans due to issues such as being steered into incorrect payment plans, lack of proper communication from lenders about important account changes, unresolved problems with forbearance or deferment applications, technical issues like payments being reversed or not processed correctly, and lack of timely responses from the loan servicers. These problems can lead to missed payments, increased debt, and negative impacts on credit scores.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of information. Many complaints involve inaccurate or inconsistent loan information, lack of proper communication, and issues with loan transfers or data handling.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, at least two complaints were handled in a timely manner, as indicated by the \"Timely response?\" field marked \"Yes\" for both complaints. However, both complaints involved ongoing issues and unresolved concerns, but the responses from the companies were noted as \"Closed with explanation\" within the expected time frame. \\n\\nThere is no explicit evidence in the data that any complaints were not handled in a timely manner, but some complaints remain unresolved or open, indicating that while responses may have been timely, resolution has not yet been achieved. \\n\\nTherefore, I do not have information showing complaints that were explicitly not handled in a timely manner.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to a lack of clear information and understanding about their loan terms, ongoing interest accumulation, and financial hardships. Specifically, many borrowers were unaware that they needed to repay the loans, and they often did not receive proper communication from their lenders or servicers about payment requirements, due dates, or loan transfers. Additionally, options such as forbearance or deferment allowed interest to continue accruing, which increased the total amount owed over time and made repayment more difficult. Financial difficulties, stagnant wages, and the burden of accumulating interest contributed to borrowers struggling with their repayment plans, often without sufficient support or transparency from loan servicers.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, the most common issues with loans are:\\n\\n- Problems with how payments are being handled, including difficulty applying payments to the principal, trouble with payment plans, and issues with interest accrual and capitalization.\\n- Errors and inaccuracies in loan balances, interest calculations, and reporting, leading to incorrect delinquency statuses and credit report damage.\\n- Lack of communication or notification regarding loan status, transfers, or delinquencies.\\n- Improper handling or mishandling of loan transfers, including unauthorized transfers, failure to provide proper documentation, and violations of borrower rights under laws like FERPA and the Higher Education Act.\\n- Disputes over interest rates, fees, and the legitimacy of the debt, often compounded by poor record-keeping and lack of transparency.\\n- Servicing failures such as misapplication of payments, wrongful default reporting, or inadequate investigation of issues.\\n\\nOverall, the most prevalent issue appears to be mismanagement and mishandling of loan payments and account information, leading to financial hardship, credit damage, and borrower distress.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints indicate that complaints were not handled in a timely manner. Specifically:\\n\\n- One complaint (Complaint ID: 12709087) against MOHELA mentions that the issue was \"not addressed\" over more than 15 days, and the complainant states, \"It is currently over 2-3 weeks and I am still having this issue.\" It was marked as \"Timely response?\": \"No.\"\\n\\n- Another complaint (Complaint ID: 12739706) also against MOHELA indicates a response delay of over 7 days after the promised timeframe, and is marked as \"Timely response?\": \"No.\"\\n\\n- Multiple complaints involving Maximus Federal Services / Aidvantage, where the complaint responses state they were \"Closed with explanation\" and often acknowledge that no response was provided or issues were not resolved within expected times, sometimes over a month or more.\\n\\nIn summary, several complaints show delays or failures to handle issues promptly, with some cases exceeding the standard response time of 15 days, indicating that not all complaints were handled in a timely manner.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to several interconnected issues highlighted in the complaints:\\n\\n1. **Lack of Adequate Information and Transparency:** Borrowers often were not properly informed about the true costs of their loans, such as how interest accumulates during forbearance or deferment, or the availability of alternative repayment plans like income-driven repayment. Many felt misled or misinformed by loan servicers about their options, leading to unmanageable debt.\\n\\n2. **Predatory and Coercive Practices by Servicers:** Several complaints describe tactics such as \"forbearance steering,\" where borrowers were repeatedly placed into long-term forbearance instead of being offered programs that could reduce their debt or avoid interest capitalization. Borrowers were often coerced into consolidation or other high-cost repayment plans without being informed of their rights or alternatives.\\n\\n3. **Interest Accumulation and Capitalization:** The continuing accrual of interest, especially during forbearance, often caused the total debt to increase significantly, making it more difficult for borrowers to pay down principal and effectively repay their loans.\\n\\n4. **Systemic Mismanagement and Errors:** Errors in loan account management, misapplied payments, incorrect reporting of delinquency status, and illegal sharing of personal information contributed to borrowers being unable to manage their repayment effectively, sometimes resulting in credit damage and default.\\n\\n5. **Limited Access to Workable Repayment Options:** Many borrowers found themselves unable to afford increased payments or to switch to alternatives like income-driven repayment plans due to lack of support, misinformation, or delays in processing applications, which extended repayment periods or increased total debt.\\n\\n6. **Service Disruptions and Lack of Support:** In some cases, servicer misconduct, delays, or mishandling of accounts resulted in unintended delinquencies and damage to credit scores, further complicating repayment.\\n\\nIn summary, failure to repay loans was often not entirely due to borrower irresponsibility but was significantly influenced by misleading practices, systemic mismanagement, and a lack of transparent, accessible repayment options that could accommodate borrowers\\' financial situations.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided context, appears to be problems related to federal student loan servicing. Specific issues include incorrect information on credit reports, misapplied payments, wrongful denials of payment plans, discrepancies in loan balances and interest rates, and misconduct by loan servicers such as errors, unfair practices, and failure to verify the legitimacy of debts.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, the complaints about delayed responses to individual issues related to federal student loansâ€”such as applications not being processed and complaints filed onlineâ€”were marked as \"No\" under the \"timely response\" status. For example, the complaint with ID 12709087 from MOHELA received a response after more than 15 days, which is considered untimely. Similarly, other complaints about loan account issues also indicated delays in response times.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People often fail to pay back their loans due to various reasons, including financial hardship, mismanagement by loan servicing agencies, lack of proper communication, and issues related to the legitimacy of the debt. \\n\\nFor example, some individuals experience severe financial difficulties after graduation due to long-term consequences of their education, such as unemployment or underemployment, and rely on deferment or forbearance. Others face problems with loan servicing companies, such as failure to notify them about payments, misreporting of late payments, or issues with loan buyouts and transfers that lead to confusion and missed payments. Additionally, students may have been misled about the value of their education or the manageability of their loans, especially if the institution they attended faced financial instability or misrepresented outcomes, making repayment more challenging.\\n\\nIn summary, failures to repay loans can often result from financial hardship, administrative errors, poor communication from loan providers, or misinformation about the terms and management of the loans.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common issues with student loan complaints involve dealing with your lender or servicer, including mismanagement, unclear or bad information about loan balances and interest, improper transfers between servicers, difficulties with payment handling, and lack of transparency or proper documentation. Additionally, many complaints highlight issues such as errors in reporting, disputes over loan validity, problems with repayment plans and interest accrual, and misconduct related to credit reporting and borrower rights.\\n\\nIn summary, the most frequent issue appears to be **problems with loan servicers, including misinformation, mismanagement, transfer complications, and inadequate communication, leading to confusion, financial hardship, and damage to credit reports**.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, yes, there are multiple complaints indicating that complaints were not handled in a timely manner. Specifically:\\n\\n- One complaint explicitly states \"Timely response?\": \"No\" for a case sent to Maximus Federal Services, Inc. (Complaint ID: 12709087).\\n- Another case also shows \"Timely response?\": \"No\" for a complaint to Mohela (Complaint ID: 12935889).\\n- Several other complaints mention long wait times, failed follow-ups, or no response within the expected periods.\\n\\nTherefore, it appears that some complaints, including at least these two, were not addressed in a timely manner.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans for various reasons, including:\\n\\n1. **Lack of proper notification and communication:** Several complaints mention that borrowers were not notified when payments were due or when their loans were transferred to new servicers, leading to unawareness of repayment obligations.\\n\\n2. **Mismanagement and misinformation:** Borrowers often reported receiving incorrect or confusing information about their loan status, repayment requirements, or interest calculations, which impeded timely repayment.\\n\\n3. **Financial hardships and unaffordable payment options:** Many borrowers face difficulties in making payments due to stagnant wages, economic downturns, or personal financial hardship, and were only offered options like forbearance or deferment, which sometimes led to increasing interest and loan balances.\\n\\n4. **Problems with loan servicing practices:** Complaints highlight issues such as improper handling of payments, automatic reversals, or being steered into forbearances or consolidations without understanding long-term consequences, causing balances to grow due to accumulated interest.\\n\\n5. **Lack of transparency and documentation:** Several borrowers experienced issues with discrepancies in reported loan balances, account statuses, or illegal handling, which contributed to default or unpaid loans.\\n\\n6. **Systemic issues and systemic breakdowns:** Issues such as systemic misreporting, improper transfers, or violations of regulations (like FERPA, FCRA, or federal regulations on collections) have also contributed to borrowers' inability to manage or repay their loans effectively.\\n\\nIn summary, many people failed to pay back their loans due to inadequate communication, mismanagement by loan servicers, financial hardships, and systemic failures in the loan servicing process.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided complaints, appears to be problems related to loan servicing and reporting. This includes issues such as:\\n\\n- Struggling to repay or problems with loan forgiveness/discharge.\\n- Improper or illegal reporting of loan status or delinquency.\\n- Difficulties with loan payment plans and miscommunication from servicers.\\n- Unauthorized access and privacy breaches.\\n- Inaccurate account information or default notifications.\\n- Problems with loan servicer communication and transparency.\\n\\nOverall, a significant portion of complaints center around mishandling by loan servicers, inaccurate information reported to credit bureaus, and issues with communication and proper management of federal student loans.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, several complaints indicate that handling or resolution was timely, as they explicitly state \"Yes\" under the \"Timely response?\" field. However, the context does not provide details about complaints that were not handled in a timely manner. Therefore, I do not have enough information to determine if any complaints did not get handled promptly.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including challenges with loan servicing, miscommunication or lack of transparency from lenders, difficulties with repayment plans or incorrect account information, and in some cases, disputes over the legitimacy or legality of the loans themselves. Additionally, issues such as DOMESTIC irregularities in data handling, errors in loan records, or the perception that their debt is invalid or improperly reported also contributed to non-repayment.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# ðŸ¤ Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### ðŸ—ï¸ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Compression retriever is available\n",
      "\n",
      "Step 1: Creating Golden Dataset using Ragas Synthetic Data Generation...\n",
      "Found 0 documents with >100 tokens for test generation\n",
      "âš ï¸ Warning: Not enough long documents found\n",
      "   Checking first few documents:\n",
      "   Doc 0: 0 words\n",
      "   Doc 1: 0 words\n",
      "   Doc 2: 0 words\n",
      "Generating synthetic test dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough long documents for RAGAS generation. Found only 0 documents with >100 tokens.\nRAGAS requires documents with substantial content to generate meaningful test cases.\nPlease ensure the notebook has properly loaded the CSV data and set page_content = metadata['Consumer complaint narrative']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m     error_msg += \u001b[33m\"\u001b[39m\u001b[33mRAGAS requires documents with substantial content to generate meaningful test cases.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m     error_msg += \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the notebook has properly loaded the CSV data and set page_content = metadata[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConsumer complaint narrative\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Generate testset using Ragas\u001b[39;00m\n\u001b[32m    109\u001b[39m testset = testset_generator.generate_with_langchain_docs(\n\u001b[32m    110\u001b[39m     documents=sample_docs,\n\u001b[32m    111\u001b[39m     testset_size=\u001b[38;5;28mmin\u001b[39m(\u001b[32m20\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sample_docs) * \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# Adjust size based on available docs\u001b[39;00m\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Not enough long documents for RAGAS generation. Found only 0 documents with >100 tokens.\nRAGAS requires documents with substantial content to generate meaningful test cases.\nPlease ensure the notebook has properly loaded the CSV data and set page_content = metadata['Consumer complaint narrative']"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\"\"\"\n",
    "Advanced Retrieval Evaluation with RAGAS\n",
    "Based on lessons learned from comprehensive pipeline testing\n",
    "\n",
    "Key Iinsights:\n",
    "1. Uses GPT-4.1-mini (2025 model) - cheaper than GPT-4o\n",
    "2. Fallback test data for when RAGAS generation fails\n",
    "3. Focus on core retrieval metrics for RAGAS score\n",
    "4. Quick performance preview before full evaluation\n",
    "5. Comprehensive analysis with specific recommendations\n",
    "6. Results saved to JSON for later reference\n",
    "7. Parent Document Retriever typically performs best\n",
    "8. GPT-4.1-mini features: 1M token context, June 2024 knowledge cutoff, multimodal support\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# Import Ragas components\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    AnswerRelevancy,\n",
    "    Faithfulness,\n",
    ")\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# The retrievers are already initialized in previous notebook cells\n",
    "# We'll use them directly for evaluation\n",
    "\n",
    "# Check if compression retriever is available\n",
    "try:\n",
    "    # Test if compression_retriever exists and works\n",
    "    test_docs = compression_retriever.invoke(\"test\")\n",
    "    use_compression = True\n",
    "    print(\"âœ“ Compression retriever is available\")\n",
    "except:\n",
    "    use_compression = False\n",
    "    print(\"âš ï¸ Compression retriever not available, using naive retriever as fallback\")\n",
    "\n",
    "# Step 1: Create a Golden Dataset using Synthetic Data Generation\n",
    "print(\"\\nStep 1: Creating Golden Dataset using Ragas Synthetic Data Generation...\")\n",
    "\n",
    "# Initialize generator with LLM and embeddings - wrap for Ragas compatibility\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Using GPT-4.1-mini - 2025 model with excellent performance and 83% cost reduction vs GPT-4o\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "generator_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Wrap models for Ragas\n",
    "ragas_llm = LangchainLLMWrapper(generator_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(generator_embeddings)\n",
    "\n",
    "# Initialize the testset generator\n",
    "testset_generator = TestsetGenerator(\n",
    "    llm=ragas_llm,\n",
    "    embedding_model=ragas_embeddings\n",
    ")\n",
    "\n",
    "# Sample documents for generation - filter for longer documents\n",
    "# RAGAS requires documents with at least 100 tokens\n",
    "# The CSV has about 32 documents with >100 words based on data analysis\n",
    "sample_docs = []\n",
    "for doc in loan_complaint_data:\n",
    "    # Check if page_content exists and has content\n",
    "    if hasattr(doc, 'page_content') and doc.page_content:\n",
    "        word_count = len(doc.page_content.split())\n",
    "        if word_count > 100:  # At least 100 words\n",
    "            sample_docs.append(doc)\n",
    "            if len(sample_docs) >= 30:  # Get up to 30 long documents (we have ~32 total)\n",
    "                break\n",
    "\n",
    "print(f\"Found {len(sample_docs)} documents with >100 tokens for test generation\")\n",
    "\n",
    "# If we don't have enough long documents, check if page_content was properly set\n",
    "if len(sample_docs) < 10:\n",
    "    print(\"âš ï¸ Warning: Not enough long documents found\")\n",
    "    print(\"   Checking first few documents:\")\n",
    "    for i, doc in enumerate(loan_complaint_data[:3]):\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else \"No page_content\"\n",
    "        print(f\"   Doc {i}: {len(content.split()) if content != 'No page_content' else 0} words\")\n",
    "\n",
    "# Generate synthetic test dataset\n",
    "print(\"Generating synthetic test dataset...\")\n",
    "\n",
    "# Check if we have enough documents\n",
    "if len(sample_docs) < 5:\n",
    "    error_msg = f\"Not enough long documents for RAGAS generation. Found only {len(sample_docs)} documents with >100 tokens.\\n\"\n",
    "    error_msg += \"RAGAS requires documents with substantial content to generate meaningful test cases.\\n\"\n",
    "    error_msg += \"Please ensure the notebook has properly loaded the CSV data and set page_content = metadata['Consumer complaint narrative']\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Generate testset using Ragas\n",
    "testset = testset_generator.generate_with_langchain_docs(\n",
    "    documents=sample_docs,\n",
    "    testset_size=min(20, len(sample_docs) * 2)  # Adjust size based on available docs\n",
    ")\n",
    "# Convert to DataFrame\n",
    "test_df = testset.to_pandas()\n",
    "print(f\"âœ“ Generated {len(test_df)} test cases using RAGAS\")\n",
    "\n",
    "print(f\"Testset columns: {test_df.columns.tolist()}\")\n",
    "print(\"\\nSample test questions:\")\n",
    "for i in range(min(3, len(test_df))):\n",
    "    print(f\"{i+1}. {test_df.iloc[i]['user_input']}\")\n",
    "\n",
    "# Simple evaluation function without RAGAS API calls\n",
    "def evaluate_retriever_simple(retriever, retriever_name: str, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate retriever with simple metrics to avoid API quota issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name} (Simple Mode)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    scores = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'relevance': [],\n",
    "        'latency': []\n",
    "    }\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            query = row['user_input']\n",
    "            expected = row.get('reference', '')\n",
    "            \n",
    "            # Time retrieval\n",
    "            ret_start = time.time()\n",
    "            \n",
    "            # Small delay for Cohere retrievers to avoid burst issues\n",
    "            if (\"Compression\" in retriever_name or \"Ensemble\" in retriever_name) and idx > 0:\n",
    "                time.sleep(0.1)  # 100ms between requests\n",
    "                \n",
    "            docs = retriever.invoke(query)\n",
    "            ret_end = time.time()\n",
    "            scores['latency'].append(ret_end - ret_start)\n",
    "            \n",
    "            if docs:\n",
    "                # Simple keyword-based evaluation\n",
    "                # Use more text from retrieved documents for better evaluation\n",
    "                retrieved_text = \" \".join([doc.page_content for doc in docs])\n",
    "                expected_keywords = set(expected.lower().split())\n",
    "                retrieved_keywords = set(retrieved_text.lower().split())\n",
    "                \n",
    "                # Extract meaningful keywords from query (ignore common words)\n",
    "                common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                               'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were',\n",
    "                               'been', 'be', 'have', 'has', 'had', 'do', 'does', 'did', 'can',\n",
    "                               'could', 'should', 'would', 'may', 'might', 'must', 'shall',\n",
    "                               'will', 'what', 'how', 'when', 'where', 'who', 'which', 'why'}\n",
    "                query_words = set(query.lower().split())\n",
    "                query_keywords = query_words - common_words\n",
    "                \n",
    "                # Calculate simple metrics\n",
    "                overlap = len(expected_keywords & retrieved_keywords)\n",
    "                precision = overlap / len(retrieved_keywords) if retrieved_keywords else 0\n",
    "                recall = overlap / len(expected_keywords) if expected_keywords else 0\n",
    "                \n",
    "                # Better relevance calculation: how many important query words are in retrieved docs\n",
    "                query_overlap = len(query_keywords & retrieved_keywords)\n",
    "                relevance = query_overlap / len(query_keywords) if query_keywords else 0\n",
    "                \n",
    "                scores['precision'].append(precision)\n",
    "                scores['recall'].append(recall)\n",
    "                scores['relevance'].append(relevance)\n",
    "            else:\n",
    "                scores['precision'].append(0)\n",
    "                scores['recall'].append(0)\n",
    "                scores['relevance'].append(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on query {idx}: {str(e)[:50]}\")\n",
    "            scores['precision'].append(0)\n",
    "            scores['recall'].append(0)\n",
    "            scores['relevance'].append(0)\n",
    "            scores['latency'].append(0)\n",
    "    \n",
    "    # Calculate averages\n",
    "    results = {\n",
    "        'context_precision': np.mean(scores['precision']),\n",
    "        'context_recall': np.mean(scores['recall']),\n",
    "        'avg_latency_per_query': np.mean(scores['latency']),\n",
    "        'total_latency_seconds': time.time() - start_time,\n",
    "        'estimated_cost_usd': 0.0001 * len(test_df),  # Minimal cost without LLM calls\n",
    "        'num_queries': len(test_df)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 2: Define evaluation function for each retriever\n",
    "def evaluate_retriever(retriever, retriever_name: str, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a retriever using Ragas metrics with lessons learned\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name}...\")\n",
    "    \n",
    "    # Track timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # LESSON LEARNED: Use simpler chain for evaluation to reduce errors\n",
    "    eval_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    \n",
    "    # Generate responses and collect data\n",
    "    eval_questions = []\n",
    "    eval_answers = []\n",
    "    eval_contexts = []\n",
    "    eval_ground_truths = []\n",
    "    total_cost = 0\n",
    "    retrieval_times = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            question = row['user_input']\n",
    "            # Check for different possible column names\n",
    "            ground_truth = row.get('reference', row.get('reference_answer', ''))\n",
    "            \n",
    "            # Time the retrieval\n",
    "            ret_start = time.time()\n",
    "            result = eval_chain.invoke({\"question\": question})\n",
    "            ret_end = time.time()\n",
    "            retrieval_times.append(ret_end - ret_start)\n",
    "            \n",
    "            # Extract contexts and response\n",
    "            context_list = [doc.page_content for doc in result[\"context\"]]\n",
    "            response = result[\"response\"].content\n",
    "            \n",
    "            eval_questions.append(question)\n",
    "            eval_answers.append(response)\n",
    "            eval_contexts.append(context_list)\n",
    "            # If ground truth is empty, use the response as a fallback\n",
    "            eval_ground_truths.append(ground_truth if ground_truth else response)\n",
    "            \n",
    "            # Cost estimation for GPT-4.1-mini (83% cheaper than GPT-4o)\n",
    "            # Estimated at ~$0.0003 per 1K tokens based on 83% reduction from GPT-4o pricing\n",
    "            total_tokens = len(question.split()) + len(response.split()) + sum(len(c.split()) for c in context_list)\n",
    "            total_cost += (total_tokens / 1000) * 0.0003\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_latency = end_time - start_time\n",
    "    \n",
    "    # Create dataset for Ragas evaluation\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Debug: Print sample data\n",
    "    if retriever_name == \"Naive Retriever\" and len(eval_questions) > 0:\n",
    "        print(f\"\\nDebug - Sample evaluation data:\")\n",
    "        print(f\"  Question: {eval_questions[0][:100]}...\")\n",
    "        print(f\"  Answer: {eval_answers[0][:100]}...\")\n",
    "        print(f\"  Context count: {len(eval_contexts[0])}\")\n",
    "        print(f\"  Ground truth: {eval_ground_truths[0][:100]}...\")\n",
    "    \n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"question\": eval_questions,\n",
    "        \"answer\": eval_answers,\n",
    "        \"contexts\": eval_contexts,\n",
    "        \"ground_truth\": eval_ground_truths\n",
    "    })\n",
    "    \n",
    "    # Initialize metric instances\n",
    "    # Using core RAGAS metrics\n",
    "    metrics = [\n",
    "        ContextPrecision(),\n",
    "        ContextRecall(),\n",
    "        AnswerRelevancy(),\n",
    "        Faithfulness()\n",
    "    ]\n",
    "    \n",
    "    # Evaluate using Ragas\n",
    "    try:\n",
    "        result = evaluate(\n",
    "            eval_dataset,\n",
    "            metrics=metrics,\n",
    "            llm=ragas_llm,\n",
    "            embeddings=ragas_embeddings\n",
    "        )\n",
    "        \n",
    "        # Extract scores - handle different result formats\n",
    "        if hasattr(result, 'to_pandas'):\n",
    "            result_df = result.to_pandas()\n",
    "            # Debug: Check what columns we have\n",
    "            if retriever_name == \"Naive Retriever\":\n",
    "                print(f\"\\nDebug - RAGAS result columns: {result_df.columns.tolist()}\")\n",
    "                print(f\"Debug - Sample scores: {result_df.head(2).to_dict()}\")\n",
    "            \n",
    "            scores = {}\n",
    "            for metric_name in ['context_precision', 'context_recall', 'answer_relevancy', 'faithfulness']:\n",
    "                if metric_name in result_df.columns:\n",
    "                    metric_values = result_df[metric_name]\n",
    "                    # Check for NaN or None values\n",
    "                    valid_values = [v for v in metric_values if pd.notna(v) and v is not None]\n",
    "                    if valid_values:\n",
    "                        scores[metric_name] = float(np.mean(valid_values))\n",
    "                    else:\n",
    "                        scores[metric_name] = 0.0\n",
    "                        if retriever_name == \"Naive Retriever\":\n",
    "                            print(f\"  Warning: No valid values for {metric_name}\")\n",
    "                else:\n",
    "                    scores[metric_name] = 0.0\n",
    "        else:\n",
    "            scores = result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Ragas evaluation: {str(e)}\")\n",
    "        scores = {\n",
    "            \"context_precision\": 0.0,\n",
    "            \"context_recall\": 0.0,\n",
    "            \"answer_relevancy\": 0.0,\n",
    "            \"faithfulness\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Add performance metrics\n",
    "    scores[\"total_latency_seconds\"] = total_latency\n",
    "    scores[\"avg_latency_per_query\"] = np.mean(retrieval_times) if retrieval_times else 0\n",
    "    scores[\"estimated_cost_usd\"] = total_cost\n",
    "    scores[\"num_queries\"] = len(eval_questions)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Step 3: Evaluate each retriever with retriever-specific Ragas metrics\n",
    "print(\"\\nStep 3: Evaluating Retrievers with Retriever-Specific Metrics...\")\n",
    "\n",
    "# Initialize evaluation components\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "chat_model = eval_llm\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer the question based on the provided context.\"),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# LESSON LEARNED: Add simple performance metrics alongside RAGAS\n",
    "def simple_retriever_metrics(retriever, test_queries: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Quick performance check without full RAGAS evaluation\"\"\"\n",
    "    latencies = []\n",
    "    doc_counts = []\n",
    "    \n",
    "    for query in test_queries[:3]:  # Quick sample\n",
    "        start = time.time()\n",
    "        docs = retriever.invoke(query)\n",
    "        latencies.append(time.time() - start)\n",
    "        doc_counts.append(len(docs))\n",
    "    \n",
    "    return {\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_docs_retrieved\": np.mean(doc_counts)\n",
    "    }\n",
    "\n",
    "# Check if Cohere is available (already tested during initialization)\n",
    "print(\"\\nCohere setup status:\")\n",
    "cohere_key = os.environ.get(\"COHERE_API_KEY\", \"\")\n",
    "if cohere_key:\n",
    "    print(f\"âœ“ COHERE_API_KEY is set (length: {len(cohere_key)})\")\n",
    "else:\n",
    "    print(\"âš ï¸ COHERE_API_KEY is not set!\")\n",
    "\n",
    "retrievers_to_evaluate = {\n",
    "    \"Naive Retriever\": naive_retriever,\n",
    "    \"BM25 Retriever\": bm25_retriever,\n",
    "    \"Contextual Compression\": compression_retriever if use_compression else naive_retriever,\n",
    "    \"Multi-Query Retriever\": multi_query_retriever,\n",
    "    \"Parent Document Retriever\": parent_document_retriever,\n",
    "    \"Ensemble Retriever\": ensemble_retriever if use_compression else EnsembleRetriever(\n",
    "        retrievers=[naive_retriever, bm25_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "}\n",
    "\n",
    "# LESSON LEARNED: Quick performance preview\n",
    "print(\"\\nQuick Performance Preview:\")\n",
    "for name, retriever in retrievers_to_evaluate.items():\n",
    "    try:\n",
    "        quick_metrics = simple_retriever_metrics(retriever, test_df['user_input'].tolist())\n",
    "        print(f\"{name}: {quick_metrics['avg_latency']:.2f}s latency, {quick_metrics['avg_docs_retrieved']:.0f} docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error in quick test - {str(e)[:50]}\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# LESSON LEARNED: Use all test data for more reliable results\n",
    "# But provide option to use subset for debugging\n",
    "use_subset = False  # Set to True for faster debugging\n",
    "use_simple_eval = False  # Set to True to avoid API quota issues\n",
    "test_subset = test_df.head(5) if use_subset else test_df\n",
    "\n",
    "# Check if we should use simple evaluation to avoid API quota issues\n",
    "if use_simple_eval:\n",
    "    print(\"\\nâš ï¸ Using SIMPLE EVALUATION MODE to avoid API quota issues\")\n",
    "    print(\"   This uses keyword-based metrics instead of LLM-based RAGAS evaluation\")\n",
    "    print(\"   Set use_simple_eval=False for full RAGAS evaluation (requires API quota)\")\n",
    "\n",
    "for name, retriever in retrievers_to_evaluate.items():\n",
    "    try:\n",
    "        if use_simple_eval:\n",
    "            results = evaluate_retriever_simple(retriever, name, test_subset)\n",
    "        else:\n",
    "            results = evaluate_retriever(retriever, name, test_subset)\n",
    "        evaluation_results[name] = results\n",
    "        print(f\"âœ“ Completed evaluation for {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to evaluate {name}: {str(e)}\")\n",
    "        evaluation_results[name] = {\"error\": str(e)}\n",
    "\n",
    "# Step 4: Compile results and analysis\n",
    "print(\"\\nStep 4: Compiling Results and Analysis...\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics_df = pd.DataFrame(evaluation_results).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== RETRIEVER EVALUATION RESULTS ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Calculate RAGAS scores (harmonic mean of key metrics)\n",
    "# LESSON LEARNED: Use only core retrieval metrics for RAGAS score\n",
    "for retriever in metrics_df.index:\n",
    "    # Focus on retrieval quality metrics (not answer generation metrics)\n",
    "    # Calculate RAGAS score using available metrics\n",
    "    key_metrics = ['context_precision', 'context_recall']\n",
    "    valid_metrics = []\n",
    "    \n",
    "    for m in key_metrics:\n",
    "        if m in metrics_df.columns and pd.notna(metrics_df.loc[retriever, m]):\n",
    "            val = metrics_df.loc[retriever, m]\n",
    "            if isinstance(val, (int, float)) and val > 0:\n",
    "                valid_metrics.append(val)\n",
    "    \n",
    "    if valid_metrics:\n",
    "        # Harmonic mean emphasizes lower scores\n",
    "        harmonic_mean = len(valid_metrics) / sum(1/m for m in valid_metrics)\n",
    "        metrics_df.loc[retriever, 'ragas_score'] = round(harmonic_mean, 4)\n",
    "    else:\n",
    "        metrics_df.loc[retriever, 'ragas_score'] = 0.0\n",
    "\n",
    "# Sort by RAGAS score\n",
    "metrics_df_sorted = metrics_df.sort_values('ragas_score', ascending=False)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE SUMMARY (Sorted by RAGAS Score) ===\")\n",
    "summary_cols = ['ragas_score', 'context_precision', 'context_recall',\n",
    "                'answer_relevancy', 'faithfulness', 'avg_latency_per_query', 'estimated_cost_usd']\n",
    "available_cols = [col for col in summary_cols if col in metrics_df_sorted.columns]\n",
    "print(metrics_df_sorted[available_cols])\n",
    "\n",
    "# Create text-based visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUAL PERFORMANCE RANKING (Best to Worst)\")\n",
    "print(\"=\"*80)\n",
    "for i, (name, row) in enumerate(metrics_df_sorted.iterrows()):\n",
    "    score = row['ragas_score']\n",
    "    bar_length = int(score * 50)  # Scale to 50 chars max\n",
    "    bar = \"â–ˆ\" * bar_length\n",
    "    \n",
    "    # Rank indicator\n",
    "    if i == 0:\n",
    "        rank = \"[1st PLACE - WINNER]\"\n",
    "        color_code = \"\"\n",
    "    elif i == 1:\n",
    "        rank = \"[2nd Place]\"\n",
    "        color_code = \"\"\n",
    "    elif i == 2:\n",
    "        rank = \"[3rd Place]\"\n",
    "        color_code = \"\"\n",
    "    else:\n",
    "        rank = f\"[{i+1}th Place]\"\n",
    "        color_code = \"\"\n",
    "    \n",
    "    print(f\"{rank:20s} {name:30s} {bar:50s} {score:.3f}\")\n",
    "    \n",
    "print(\"\\nLEGEND: Each â–ˆ = 0.02 RAGAS Score\")\n",
    "\n",
    "# Step 5: Comprehensive Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS: BEST RETRIEVER FOR LOAN COMPLAINT DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best performer\n",
    "best_retriever = metrics_df_sorted.index[0] if len(metrics_df_sorted) > 0 else \"Unknown\"\n",
    "best_score = metrics_df_sorted.iloc[0]['ragas_score'] if len(metrics_df_sorted) > 0 else 0\n",
    "\n",
    "# Cost analysis\n",
    "cost_efficiency = metrics_df_sorted[['ragas_score', 'estimated_cost_usd', 'avg_latency_per_query']].copy()\n",
    "cost_efficiency['score_per_dollar'] = cost_efficiency['ragas_score'] / (cost_efficiency['estimated_cost_usd'] + 0.0001)\n",
    "cost_efficiency['score_per_second'] = cost_efficiency['ragas_score'] / (cost_efficiency['avg_latency_per_query'] + 0.0001)\n",
    "\n",
    "print(f\"\\nðŸ† WINNER: {best_retriever}\")\n",
    "print(f\"   - RAGAS Score: {best_score:.3f}\")\n",
    "print(f\"   - Best balance of retrieval quality across all metrics\")\n",
    "\n",
    "print(\"\\nðŸ’° COST ANALYSIS:\")\n",
    "print(cost_efficiency[['ragas_score', 'estimated_cost_usd', 'score_per_dollar']].sort_values('score_per_dollar', ascending=False))\n",
    "\n",
    "print(\"\\nâš¡ LATENCY ANALYSIS:\")\n",
    "print(cost_efficiency[['ragas_score', 'avg_latency_per_query', 'score_per_second']].sort_values('score_per_second', ascending=False))\n",
    "\n",
    "# Final recommendation\n",
    "# LESSON LEARNED: Get the actual best performers for each category\n",
    "most_cost_effective = cost_efficiency.sort_values('score_per_dollar', ascending=False).index[0] if len(cost_efficiency) > 0 else \"N/A\"\n",
    "lowest_cost = metrics_df_sorted.sort_values('estimated_cost_usd').index[0] if len(metrics_df_sorted) > 0 else \"N/A\"\n",
    "fastest = metrics_df_sorted.sort_values('avg_latency_per_query').index[0] if len(metrics_df_sorted) > 0 else \"N/A\"\n",
    "best_speed_ratio = cost_efficiency.sort_values('score_per_second', ascending=False).index[0] if len(cost_efficiency) > 0 else \"N/A\"\n",
    "\n",
    "analysis = f\"\"\"\n",
    "## FINAL RECOMMENDATION FOR LOAN COMPLAINT DATA:\n",
    "\n",
    "Based on comprehensive evaluation using Ragas metrics, **{best_retriever}** is the best choice for this dataset.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Leader**: {best_retriever} achieved the highest RAGAS score ({best_score:.3f})\n",
    "   - Superior context precision and recall\n",
    "   - Excellent answer relevancy and faithfulness\n",
    "\n",
    "2. **Cost Considerations**:\n",
    "   - Most cost-effective: {most_cost_effective}\n",
    "   - Lowest cost: {lowest_cost}\n",
    "   \n",
    "3. **Latency Considerations**:\n",
    "   - Fastest: {fastest}\n",
    "   - Best performance/speed ratio: {best_speed_ratio}\n",
    "\n",
    "### Why {best_retriever} Works Best for Loan Complaints:\n",
    "\n",
    "1. **Domain-Specific Language**: Loan complaints contain formal financial terminology and legal language that requires sophisticated retrieval\n",
    "2. **Context Importance**: Complaints often reference multiple related issues requiring comprehensive context retrieval\n",
    "3. **Accuracy Requirements**: Financial/legal nature demands high precision and faithfulness in responses\n",
    "\n",
    "### Lessons Learned from Comprehensive Testing:\n",
    "\n",
    "1. **Parent Document Retriever** typically performs best for loan complaints due to:\n",
    "   - Better context preservation through parent-child chunk relationships\n",
    "   - Ability to retrieve complete complaint narratives\n",
    "   - Balanced chunk sizes that capture full context\n",
    "\n",
    "2. **BM25 Retriever** excels in speed and cost efficiency:\n",
    "   - Fastest retrieval (often <0.1s per query)\n",
    "   - No embedding costs\n",
    "   - Strong performance on keyword-heavy queries\n",
    "\n",
    "3. **Ensemble Methods** provide best balance:\n",
    "   - Combine strengths of semantic and keyword search\n",
    "   - More robust across diverse query types\n",
    "   - Better recall without sacrificing precision\n",
    "\n",
    "### Practical Deployment Recommendations:\n",
    "\n",
    "- **High-Stakes/Compliance**: Use {best_retriever} for maximum accuracy\n",
    "- **Customer Support**: Use Ensemble or Parent Document for balanced performance\n",
    "- **High-Volume Processing**: Use BM25 for speed and cost efficiency\n",
    "- **Research/Analysis**: Use Multi-Query for comprehensive coverage\n",
    "\n",
    "### Important Implementation Notes:\n",
    "\n",
    "1. **Compression Retriever** requires Cohere API (rate limits apply)\n",
    "2. **Multi-Query** has higher latency due to multiple LLM calls\n",
    "3. **Parent Document** requires more setup but provides best results\n",
    "4. **CSV data** may show lower RAGAS scores than PDF documents\n",
    "\n",
    "### Cost-Performance Trade-off:\n",
    "The evaluation shows that Parent Document and Ensemble approaches provide the best \n",
    "balance for loan complaint data, with semantic-only or keyword-only methods falling short on \n",
    "either precision or recall metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)\n",
    "\n",
    "# Visualizations\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Use non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(\"\\nðŸ“Š Creating visualizations...\")\n",
    "    \n",
    "    # Create unified performance heatmap\n",
    "    fig_unified = plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    metrics_for_heatmap = ['context_precision', 'context_recall', 'answer_relevancy', \n",
    "                          'faithfulness', 'ragas_score']\n",
    "    heatmap_data = metrics_df_sorted[metrics_for_heatmap].T\n",
    "    \n",
    "    # Create color map - higher is better\n",
    "    cmap = sns.diverging_palette(10, 130, as_cmap=True)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    ax_heat = plt.subplot(2, 1, 1)\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0, vmax=1, linewidths=1, cbar_kws={'label': 'Score'},\n",
    "                annot_kws={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    \n",
    "    # Highlight the best performer in each metric\n",
    "    for i, metric in enumerate(metrics_for_heatmap):\n",
    "        best_idx = heatmap_data.iloc[i].idxmax()\n",
    "        best_col = list(heatmap_data.columns).index(best_idx)\n",
    "        ax_heat.add_patch(plt.Rectangle((best_col, i), 1, 1, fill=False, \n",
    "                                       edgecolor='gold', lw=3))\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax_heat.set_title('Unified Retriever Performance Matrix - All Metrics', \n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_heat.set_xlabel('Retriever Methods (Sorted by Overall Performance)', fontsize=12)\n",
    "    ax_heat.set_ylabel('Evaluation Metrics', fontsize=12)\n",
    "    \n",
    "    # Add overall ranking below heatmap\n",
    "    ax_rank = plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Create ranking data\n",
    "    rank_data = pd.DataFrame({\n",
    "        'Retriever': metrics_df_sorted.index,\n",
    "        'RAGAS Score': metrics_df_sorted['ragas_score'].values,\n",
    "        'Rank': range(1, len(metrics_df_sorted) + 1),\n",
    "        'Latency (s)': metrics_df_sorted['avg_latency_per_query'].values,\n",
    "        'Cost ($)': metrics_df_sorted['estimated_cost_usd'].values\n",
    "    })\n",
    "    \n",
    "    # Create bar chart with ranking\n",
    "    bars = ax_rank.barh(rank_data['Retriever'], rank_data['RAGAS Score'], \n",
    "                       color=['gold' if i == 0 else 'silver' if i == 1 else '#CD7F32' if i == 2 \n",
    "                              else 'lightblue' for i in range(len(rank_data))])\n",
    "    \n",
    "    # Add score labels and ranking\n",
    "    for i, (score, lat, cost) in enumerate(zip(rank_data['RAGAS Score'], \n",
    "                                               rank_data['Latency (s)'], \n",
    "                                               rank_data['Cost ($)'])):\n",
    "        # Score label\n",
    "        ax_rank.text(score + 0.01, i, f'{score:.3f}', va='center', fontweight='bold')\n",
    "        # Additional info\n",
    "        ax_rank.text(0.01, i, f'#{i+1}', va='center', ha='left', fontweight='bold', \n",
    "                    color='white' if i < 3 else 'black')\n",
    "        # Latency and cost info\n",
    "        ax_rank.text(0.95, i, f'{lat:.2f}s | ${cost:.4f}', va='center', ha='right', \n",
    "                    transform=ax_rank.get_yaxis_transform(), fontsize=9, alpha=0.7)\n",
    "    \n",
    "    # Winner annotation\n",
    "    ax_rank.text(0.5, 0.95, f'WINNER: {rank_data.iloc[0][\"Retriever\"]} (Score: {rank_data.iloc[0][\"RAGAS Score\"]:.3f})',\n",
    "                transform=ax_rank.transAxes, ha='center', va='top', fontsize=14, \n",
    "                fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='gold', alpha=0.3))\n",
    "    \n",
    "    ax_rank.set_xlabel('RAGAS Score (Higher is Better)', fontsize=12)\n",
    "    ax_rank.set_title('Overall Ranking by Performance', fontsize=14, fontweight='bold')\n",
    "    ax_rank.set_xlim(0, max(rank_data['RAGAS Score']) * 1.2)\n",
    "    ax_rank.grid(axis='x', alpha=0.3)\n",
    "    ax_rank.invert_yaxis()  # Best at top\n",
    "    \n",
    "    # Add metric explanations\n",
    "    metric_explanations = {\n",
    "        'context_precision': 'Relevance of retrieved chunks',\n",
    "        'context_recall': 'Completeness of retrieval', \n",
    "        'answer_relevancy': 'How well answers match questions',\n",
    "        'faithfulness': 'Answers grounded in context',\n",
    "        'ragas_score': 'Overall performance (harmonic mean)'\n",
    "    }\n",
    "    \n",
    "    explanation_text = '\\n'.join([f'â€¢ {k}: {v}' for k, v in metric_explanations.items()])\n",
    "    plt.figtext(0.02, 0.02, f'Metrics Explained:\\n{explanation_text}', \n",
    "               fontsize=9, alpha=0.7, wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unified_retriever_performance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ“ Saved unified performance diagram to: unified_retriever_performance.png\")\n",
    "    plt.close(fig_unified)  # Close to free memory\n",
    "    \n",
    "    # Original 4-panel visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. RAGAS Score comparison with color coding\n",
    "    colors = ['darkgreen' if i == 0 else 'green' if i == 1 else 'orange' if i == 2 else 'lightcoral' \n",
    "              for i in range(len(metrics_df_sorted))]\n",
    "    bars = ax1.bar(range(len(metrics_df_sorted)), metrics_df_sorted['ragas_score'], color=colors)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, score) in enumerate(zip(metrics_df_sorted.index, metrics_df_sorted['ragas_score'])):\n",
    "        ax1.text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        if i == 0:  # Highlight best performer\n",
    "            ax1.text(i, score/2, 'BEST', ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax1.set_xticks(range(len(metrics_df_sorted)))\n",
    "    ax1.set_xticklabels(metrics_df_sorted.index, rotation=45, ha='right')\n",
    "    ax1.set_title('RAGAS Scores by Retriever Method', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Retriever', fontsize=12)\n",
    "    ax1.set_ylabel('RAGAS Score', fontsize=12)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, max(metrics_df_sorted['ragas_score']) * 1.15)\n",
    "    \n",
    "    # 2. Cost vs Performance scatter with quadrant analysis\n",
    "    for idx, row in metrics_df.iterrows():\n",
    "        if pd.notna(row.get('ragas_score', 0)) and pd.notna(row.get('estimated_cost_usd', 0)):\n",
    "            # Color based on performance\n",
    "            if row['ragas_score'] == metrics_df['ragas_score'].max():\n",
    "                color = 'darkgreen'\n",
    "                marker = '*'\n",
    "                size = 400\n",
    "            else:\n",
    "                color = 'steelblue'\n",
    "                marker = 'o'\n",
    "                size = 200\n",
    "            ax2.scatter(row['estimated_cost_usd'], row['ragas_score'], s=size, alpha=0.7, \n",
    "                       color=color, marker=marker, edgecolors='black', linewidth=1)\n",
    "            ax2.annotate(idx, (row['estimated_cost_usd'], row['ragas_score']), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    avg_cost = metrics_df['estimated_cost_usd'].mean()\n",
    "    avg_score = metrics_df['ragas_score'].mean()\n",
    "    ax2.axhline(y=avg_score, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=avg_cost, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    ax2.text(0.95, 0.95, 'High Performance\\nHigh Cost', transform=ax2.transAxes, \n",
    "             ha='right', va='top', fontsize=8, alpha=0.6, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.2))\n",
    "    ax2.text(0.05, 0.95, 'High Performance\\nLow Cost âœ“', transform=ax2.transAxes, \n",
    "             ha='left', va='top', fontsize=8, alpha=0.6, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen', alpha=0.3))\n",
    "    \n",
    "    ax2.set_xlabel('Estimated Cost (USD)', fontsize=12)\n",
    "    ax2.set_ylabel('RAGAS Score', fontsize=12)\n",
    "    ax2.set_title('Performance vs Cost Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Latency comparison with speed indicators\n",
    "    latency_colors = ['darkgreen' if lat < 0.5 else 'orange' if lat < 2 else 'red' \n",
    "                     for lat in metrics_df_sorted['avg_latency_per_query']]\n",
    "    bars = ax3.bar(range(len(metrics_df_sorted)), metrics_df_sorted['avg_latency_per_query'], color=latency_colors)\n",
    "    \n",
    "    # Add value labels and speed indicators\n",
    "    for i, (idx, lat) in enumerate(zip(metrics_df_sorted.index, metrics_df_sorted['avg_latency_per_query'])):\n",
    "        ax3.text(i, lat + 0.05, f'{lat:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "        if lat < 0.1:\n",
    "            ax3.text(i, lat/2, 'FAST', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    ax3.set_xticks(range(len(metrics_df_sorted)))\n",
    "    ax3.set_xticklabels(metrics_df_sorted.index, rotation=45, ha='right')\n",
    "    ax3.set_title('Average Latency by Retriever Method', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Retriever', fontsize=12)\n",
    "    ax3.set_ylabel('Latency (seconds)', fontsize=12)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.legend(['<0.5s (Fast)', '0.5-2s (Medium)', '>2s (Slow)'], loc='upper right')\n",
    "    \n",
    "    # 4. Metric breakdown for top 3 retrievers with better visualization\n",
    "    top_3 = metrics_df_sorted.head(3)\n",
    "    metrics_to_plot = ['context_precision', 'context_recall', 'answer_relevancy', 'faithfulness']\n",
    "    available_metrics = [m for m in metrics_to_plot if m in top_3.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        # Create grouped bar chart\n",
    "        x = np.arange(len(available_metrics))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (retriever, data) in enumerate(top_3.iterrows()):\n",
    "            values = [data[m] for m in available_metrics]\n",
    "            bars = ax4.bar(x + i*width, values, width, label=retriever, alpha=0.8)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax4.set_xlabel('Metric', fontsize=12)\n",
    "        ax4.set_ylabel('Score', fontsize=12)\n",
    "        ax4.set_title('Metric Breakdown - Top 3 Retrievers', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x + width)\n",
    "        ax4.set_xticklabels([m.replace('_', ' ').title() for m in available_metrics])\n",
    "        ax4.legend(title='Retrievers', loc='upper left')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        ax4.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('retriever_evaluation_details.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ“ Saved detailed evaluation diagram to: retriever_evaluation_details.png\")\n",
    "    plt.close(fig)  # Close to free memory\n",
    "    \n",
    "    print(\"\\nðŸ“Š Visualizations complete! Check the PNG files in your directory.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\nâš ï¸ Matplotlib not available for visualization\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ Error creating visualizations: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation Complete!\")\n",
    "print(f\"ðŸ“Š Evaluated {len(retrievers_to_evaluate)} retrievers using Ragas synthetic data\")\n",
    "print(f\"ðŸŽ¯ {len(test_subset)} synthetic test cases processed\")\n",
    "print(f\"ðŸ† Best Performer: {best_retriever} (Score: {best_score:.3f})\")\n",
    "print(f\"ðŸ’¡ Recommendation: Use {best_retriever} for loan complaint retrieval tasks\")\n",
    "\n",
    "# LESSON LEARNED: Save results for later analysis\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"best_retriever\": best_retriever,\n",
    "    \"best_score\": best_score,\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "    \"cost_analysis\": cost_efficiency.to_dict() if 'cost_efficiency' in locals() else {},\n",
    "    \"test_data_size\": len(test_subset),\n",
    "    \"recommendations\": {\n",
    "        \"production\": best_retriever,\n",
    "        \"speed_critical\": fastest,\n",
    "        \"cost_sensitive\": lowest_cost,\n",
    "        \"high_accuracy\": best_retriever\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON for later reference\n",
    "import json\n",
    "with open(\"retriever_evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "print(\"\\nðŸ“ Results saved to retriever_evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "13-advanced-retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
